# url = "https://www.bruegel.org/sites/default/files/2022-12/Gas%20tracker%2013.12.22.zip"
#######
# 15.12 Automate it
#######
# read the html
html = rvest::read_html("https://www.bruegel.org/dataset/european-natural-gas-imports")
# find the link
path = html_attr(html_elements(html, 'a[title="Download data"]'), "href")
url = paste0("https://www.bruegel.org/", path)
# download zip ------------------------------------------------------------
download_path = tempfile()
download_dir = dirname(download_path)
# download.file(url, download_path, method="libcurl", headers = c("User-Agent" = "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:104.0) Gecko/20100101 Firefox/104.0"))
# Download the data in loop -----------------------------------------------
success = FALSE
i = 1
while(!success & i <= 5) {
print(paste0("Try for the ", i, ". time"))
# try the download
res = httr::GET(url, write_disk(download_path, overwrite = T))
print(paste0("res: ", res$status_code))
# if it did work
if (res$status_code == 200) {
success = TRUE
}
# if it did not work
i = i + 1
}
if(!res$status_code == 200) stop("Download did not work after 5 tries")
unzip(download_path, exdir = download_dir)
# data dir ----------------------------------------------------------------
# the unzipped file does not contain the files, but another directory which apparently
# changes each week its name. But every week so far the word "Gas" was in it
unzipped_dir_name = dir(download_dir, ".*[gG]as.*", full.names = T)
# find file
files = dir(unzipped_dir_name, "*.csv|xlsx", full.names = T)
print(paste0("files: ", files))
file_idx = which(grepl("country_data", files))
# find the file -----------------------------------------------------------
file = files[[file_idx]]
fileEnding = strsplit(basename(file), "\\.") %>% .[[1]] %>% .[[2]]
# read data
if(fileEnding == "csv") {
data = read.csv(files[[file_idx]])
} else{
data = readxl::read_xlsx(file)
}
data %>%
mutate(
across(where(is.character),
~as.numeric(gsub(",", "", .x))
)
) -> data
# wrangle it a bit
col_indices = which(grepl("week|Russia_", names(data)))
data = data[, col_indices]
# data = data[complete.cases(data), ]
data[["EU Ziel"]] = (1/3) * data$Russia_2021
print(head(data))
print(tail(data))
names(data)[names(data) == "Russia_avg"] = "Durchschnitt 2015 - 2022"
names(data)[names(data) == "Russia_2022"] = "2022"
output_file = "R/output/natural_gas_russia_europe.csv"
dir = dirname(output_file); if(!dir.exists(dir)) dir.create(dir, recursive = T)
write.csv(data, output_file)
data
data
library(httr)
library(rvest)
library(dplyr)
# read the html
html = rvest::read_html("https://www.bruegel.org/dataset/european-natural-gas-imports")
# find the link
path = html_attr(html_elements(html, 'a[title="Download data"]'), "href")
url = paste0("https://www.bruegel.org/", path)
url
# download zip ------------------------------------------------------------
download_path = tempfile()
download_dir = dirname(download_path)
# Download the data in loop -----------------------------------------------
success = FALSE
i = 1
while(!success & i <= 5) {
print(paste0("Try for the ", i, ". time"))
# try the download
res = httr::GET(url, write_disk(download_path, overwrite = T))
print(paste0("res: ", res$status_code))
# if it did work
if (res$status_code == 200) {
success = TRUE
}
# if it did not work
i = i + 1
}
if(!res$status_code == 200) stop("Download did not work after 5 tries")
unzip(download_path, exdir = download_dir)
# the unzipped file does not contain the files, but another directory which apparently
# changes each week its name. But every week so far the word "Gas" was in it
unzipped_dir_name = dir(download_dir, ".*[gG]as.*", full.names = T)
# find file
files = dir(unzipped_dir_name, "*.csv|xlsx", full.names = T)
print(paste0("files: ", files))
file_idx = which(grepl("country_data", files))
# find the file -----------------------------------------------------------
file = files[[file_idx]]
fileEnding = strsplit(basename(file), "\\.") %>% .[[1]] %>% .[[2]]
# read data
if(fileEnding == "csv") {
data = read.csv(files[[file_idx]])
} else{
data = readxl::read_xlsx(file)
}
data %>%
mutate(
across(where(is.character),
~as.numeric(gsub(",", "", .x))
)
) -> data
data
# wrangle it a bit
col_indices = which(grepl("week|Russia_", names(data)))
data = data[, col_indices]
data
data[["EU Ziel"]] = (1/3) * data$Russia_2021
print(head(data))
print(tail(data))
names(data)[names(data) == "Russia_avg"] = "Durchschnitt 2015 - 2022"
names(data)[names(data) == "Russia_2022"] = "2022"
output_file = "R/output/natural_gas_russia_europe.csv"
dir = dirname(output_file); if(!dir.exists(dir)) dir.create(dir, recursive = T)
write.csv(data, output_file)
library(httr)
library(rvest)
library(dplyr)
# read the html
html = rvest::read_html("https://www.bruegel.org/dataset/european-natural-gas-imports")
# find the link
path = html_attr(html_elements(html, 'a[title="Download data"]'), "href")
url = paste0("https://www.bruegel.org/", path)
url
# download zip ------------------------------------------------------------
download_path = tempfile()
download_dir = dirname(download_path)
# Download the data in loop -----------------------------------------------
success = FALSE
i = 1
while(!success & i <= 5) {
print(paste0("Try for the ", i, ". time"))
# try the download
res = httr::GET(url, write_disk(download_path, overwrite = T))
print(paste0("res: ", res$status_code))
# if it did work
if (res$status_code == 200) {
success = TRUE
}
# if it did not work
i = i + 1
}
if(!res$status_code == 200) stop("Download did not work after 5 tries")
unzip(download_path, exdir = download_dir)
# the unzipped file does not contain the files, but another directory which apparently
# changes each week its name. But every week so far the word "Gas" was in it
unzipped_dir_name = dir(download_dir, ".*[gG]as.*", full.names = T)
# find file
files = dir(unzipped_dir_name, "*.csv|xlsx", full.names = T)
files
if(!res$status_code == 200) stop("Download did not work after 5 tries")
unzip(download_path, exdir = download_dir)
exdir
download_dir
# find file
# files = dir(unzipped_dir_name, "*.csv|xlsx", full.names = T)
files = dir(download_dir, ".*\\.csv|xlsx$", full.names = T)
files
print(paste0("files: ", files))
file_idx = which(grepl("country_data", files))
file_idx
# find the file -----------------------------------------------------------
file = files[[file_idx]]
file
fileEnding = strsplit(basename(file), "\\.") %>% .[[1]] %>% .[[2]]
# read data
if(fileEnding == "csv") {
data = read.csv(files[[file_idx]])
} else{
data = readxl::read_xlsx(file)
}
data %>%
mutate(
across(where(is.character),
~as.numeric(gsub(",", "", .x))
)
) -> data
data
# wrangle it a bit
col_indices = which(grepl("week|Russia_", names(data)))
data = data[, col_indices]
data[["EU Ziel"]] = (1/3) * data$Russia_2021
print(head(data))
print(tail(data))
names(data)[names(data) == "Russia_avg"] = "Durchschnitt 2015 - 2022"
names(data)[names(data) == "Russia_2022"] = "2022"
library(httr)
library(rvest)
library(dplyr)
# urls ---------------------------------------------------------------------
# url = "https://www.bruegel.org/sites/default/files/2022-09/gas_tracker_update_.zip"
# 10.10
# url = "https://www.bruegel.org/sites/default/files/2022-10/gas%20datasets_0.zip"
# 20.10
# url = "https://www.bruegel.org/sites/default/files/2022-10/Gas%20tracker_0.zip"
# 3.11
# url = "https://www.bruegel.org/sites/default/files/2022-11/Naturalgasimports-021122.zip"
# 14.11
# url = "https://www.bruegel.org/sites/default/files/2022-11/Gas%20tracker.zip"
# 18.11
# url = "https://www.bruegel.org/sites/default/files/2022-11/Gas%20tracker%2015%20Nov.zip"
# 28.11
# url = "https://www.bruegel.org/sites/default/files/2022-11/Gas%20tracker%2022.11.22.zip"
# 2.12
# url = "https://www.bruegel.org/sites/default/files/2022-11/Gas%20tracker%2029.11.22.zip"
# 9.12
# url = "https://www.bruegel.org/sites/default/files/2022-12/Gas%20tracker%2007.12.22.zip"
# 15.12
# url = "https://www.bruegel.org/sites/default/files/2022-12/Gas%20tracker%2013.12.22.zip"
#######
# 15.12 Automate it
#######
# read the html
html = rvest::read_html("https://www.bruegel.org/dataset/european-natural-gas-imports")
# find the link
path = html_attr(html_elements(html, 'a[title="Download data"]'), "href")
url = paste0("https://www.bruegel.org/", path)
# download zip ------------------------------------------------------------
download_path = tempfile()
download_dir = dirname(download_path)
# download.file(url, download_path, method="libcurl", headers = c("User-Agent" = "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:104.0) Gecko/20100101 Firefox/104.0"))
# Download the data in loop -----------------------------------------------
success = FALSE
i = 1
while(!success & i <= 5) {
print(paste0("Try for the ", i, ". time"))
# try the download
res = httr::GET(url, write_disk(download_path, overwrite = T))
print(paste0("res: ", res$status_code))
# if it did work
if (res$status_code == 200) {
success = TRUE
}
# if it did not work
i = i + 1
}
if(!res$status_code == 200) stop("Download did not work after 5 tries")
unzip(download_path, exdir = download_dir)
# data dir ----------------------------------------------------------------
# the unzipped file does not contain the files, but another directory which apparently
# changes each week its name. But every week so far the word "Gas" was in it
# unzipped_dir_name = dir(download_dir, ".*[gG]as.*", full.names = T)
# find file
# files = dir(unzipped_dir_name, "*.csv|xlsx", full.names = T)
files = dir(download_dir, ".*\\.csv|xlsx$", full.names = T)
print(paste0("files: ", files))
file_idx = which(grepl("country_data", files))
# find the file -----------------------------------------------------------
file = files[[file_idx]]
fileEnding = strsplit(basename(file), "\\.") %>% .[[1]] %>% .[[2]]
# read data
if(fileEnding == "csv") {
data = read.csv(files[[file_idx]])
} else{
data = readxl::read_xlsx(file)
}
data %>%
mutate(
across(where(is.character),
~as.numeric(gsub(",", "", .x))
)
) -> data
# wrangle it a bit
col_indices = which(grepl("week|Russia_", names(data)))
data = data[, col_indices]
# data = data[complete.cases(data), ]
data[["EU Ziel"]] = (1/3) * data$Russia_2021
print(head(data))
print(tail(data))
names(data)[names(data) == "Russia_avg"] = "Durchschnitt 2015 - 2022"
names(data)[names(data) == "Russia_2022"] = "2022"
output_file = "R/output/natural_gas_russia_europe.csv"
dir = dirname(output_file); if(!dir.exists(dir)) dir.create(dir, recursive = T)
write.csv(data, output_file)
library(httr)
library(rvest)
library(dplyr)
# read the html
html = rvest::read_html("https://www.bruegel.org/dataset/european-natural-gas-imports")
# find the link
path = html_attr(html_elements(html, 'a[title="Download data"]'), "href")
url = paste0("https://www.bruegel.org/", path)
url
# download zip ------------------------------------------------------------
download_path = tempfile()
download_dir = dirname(download_path)
# Download the data in loop -----------------------------------------------
success = FALSE
i = 1
while(!success & i <= 5) {
print(paste0("Try for the ", i, ". time"))
# try the download
res = httr::GET(url, write_disk(download_path, overwrite = T))
print(paste0("res: ", res$status_code))
# if it did work
if (res$status_code == 200) {
success = TRUE
}
# if it did not work
i = i + 1
}
if(!res$status_code == 200) stop("Download did not work after 5 tries")
unzip(download_path, exdir = download_dir)
# find file
# files = dir(unzipped_dir_name, "*.csv|xlsx", full.names = T)
files = dir(download_dir, ".*\\.csv|xlsx$", full.names = T)
files
download_dir
# find file
# files = dir(unzipped_dir_name, "*.csv|xlsx", full.names = T)
files = dir(download_dir, ".*\\.csv|xlsx$", full.names = T, recursive = T)
files
print(paste0("files: ", files))
file_idx = which(grepl("country_data", files))
file_idx
# find the file -----------------------------------------------------------
file = files[[file_idx]]
fileEnding = strsplit(basename(file), "\\.") %>% .[[1]] %>% .[[2]]
# read data
if(fileEnding == "csv") {
data = read.csv(files[[file_idx]])
} else{
data = readxl::read_xlsx(file)
}
data %>%
mutate(
across(where(is.character),
~as.numeric(gsub(",", "", .x))
)
) -> data
data
# wrangle it a bit
col_indices = which(grepl("week|Russia_", names(data)))
data = data[, col_indices]
data[["EU Ziel"]] = (1/3) * data$Russia_2021
print(head(data))
print(tail(data))
names(data)[names(data) == "Russia_avg"] = "Durchschnitt 2015 - 2022"
names(data)[names(data) == "Russia_2022"] = "2022"
data
output_file = "R/output/natural_gas_russia_europe.csv"
dir = dirname(output_file); if(!dir.exists(dir)) dir.create(dir, recursive = T)
write.csv(data, output_file)
library(purrr)
l = list(a = c(1,2), b=c(3,4), c=c(5,6))
l
pmap(l, \(f,s,t){
print(paste0(f,s,t))
})
pmap(l, function(f,s,t){
print(paste0(f,s,t))
})
pmap(l, function(f,s,t){
print(f)
})
l[[1]]
l[[1]][[1]]
l = list(a = list(1,2), b=list(3,4), c=list(5,6))
l
pmap(l, function(f,s,t){
print(f)
})
l
l = list(list(1,2),list(3,4),list(5,6))
pmap(l, function(f,s,t){
print(f)
})
pmap(l, function(f,s,t){
print(paste0(f,s,l))
})
l = list(
c(1, 3, 4),
c(5, 6, 7),
c(8, 3, 9))
l
print(paste0(f,s,l))
pmap(l, function(f,s,t){
print(paste0(f,s,l))
})
pmap_dbl(
.l = list(
c(1, 3, 4),
c(5, 6, 7),
c(8, 3, 9)),
.f = \(el1, el2, el3) {
print(glue("{el1} plus {el2} plus {el3} equals = {el1 + el2 + el3}"))
el1 + el2 + el3
}
)
library(glue)
pmap_dbl(
.l = list(
c(1, 3, 4),
c(5, 6, 7),
c(8, 3, 9)),
.f = \(el1, el2, el3) {
print(glue("{el1} plus {el2} plus {el3} equals = {el1 + el2 + el3}"))
el1 + el2 + el3
}
)
pmap(l, function(f,s,t){
print(glue("{f}-{s}-{t}"))
})
s = pmap(l, function(f,s,t){
print(glue("{f}-{s}-{t}"))
})
s
s = pmap(l, function(f,s,t){
print(glue("{f}-{s}-{t}"))
}) %>% unlist
s
library(dplyr)
# url ---------------------------------------------------------------------
url = "https://ec.europa.eu/energy/observatory/reports/latest_prices_with_taxes.xlsx"
# download file -----------------------------------------------------------
file = paste0(tempdir(), "/", "temp.xlsx")
download.file(url, file)
data = readxl::read_xlsx(file, sheet = 2)
# get the date (in col 5 header) ------------------------------------------------------------
col5_header = names(data)[[5]]
date_unformatted = gsub(".*\\s(\\d{1,2}\\/\\d{1,2}/\\d{1,2})$", "\\1", col5_header)
date = as.Date(date_unformatted, format="%m/%d/%y")
# format data --------------------------------------------------------------
new_names = c("country", "super95", "diesel", "heizöl")
data = data[,1:length(new_names)]
names(data) = new_names
data = data[!is.na(data$country) & !is.na(data$diesel), ]
new_data = lapply(seq_along(data), function(i){
if(i != 1){
ret = as.numeric(gsub(",", "", data[[i]]))
}else{
ret = data[[i]]
}
ret
})
new_data
new_data = as.data.frame(new_data)
new_data = setNames(new_data, names(data))
new_data = new_data[1:27, ]
new_data[["date"]] = date
new_data
# replace Czech Rep. ------------------------------------------------------
new_data$country = ifelse(new_data$country == "Czechia", "Czech Rep.", new_data$country)
# make german names -------------------------------------------------------
res = sapply(new_data$country, function(x) {
switch(
x,
"Austria" = "Österreich",
"Belgium" = "Belgien",
"Bulgaria" = "Bulgarien",
"Croatia" = "Kroatien",
"Cyprus" = "Zypern",
"Denmark" = "Dänemark",
"Estonia" = "Estland",
"Finland" = "Finnland",
"France" = "Frankreich",
"Germany" = "Deutschland",
"Greece" = "Griechenland",
"Hungary" = "Ungarn",
"Czech Rep." = "Tschechien",
"Ireland" = "Irland",
"Italy" = "Italien",
"Latvia" = "Lettland",
"Lithuania" = "Litauen",
"Luxembourg" = "Luxemburg",
"Malta" = "Malta",
"Netherlands" = "Niederlande",
"Poland" = "Polen",
"Portugal" = "Portugal",
"Romania" = "Rumänien",
"Slovakia" = "Slowakei",
"Slovenia" = "Slowenien",
"Spain" = "Spanien",
"Sweden" = "Schweden",
)
})
res
new_data[["country_ger"]] = res
new_data %>%
mutate(across(where(is.numeric),
~ round(.x / 1000, 2))) -> data_per_l
data_per_l
# use commas instead of points --------------------------------------------
data_per_l %>%
mutate(across(where(is.double),
~ gsub("\\.", ",", .x))) %>%
mutate(
diesel_tooltip = diesel
) -> data_final
data_final
# save data ---------------------------------------------------------------
fn = here::here("R/output/weekly_fuel_prices/weeky_fuel_prices.csv")
print(paste0("Filename: ", fn))
dir = dirname(fn); if(!dir.exists(dir)) dir.create(dir, recursive = T)
print(paste0("Created: ", dir))
write.csv(data_final, fn)
