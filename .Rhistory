ret = as.numeric(gsub(",", "", data[[i]]))
}else{
ret = data[[i]]
}
ret
})
new_data = as.data.frame(new_data)
new_data = setNames(new_data, names(data))
new_data = new_data[1:27, ]
new_data[["date"]] = date
# replace Czech Rep. ------------------------------------------------------
new_data$country = ifelse(new_data$country == "Czechia", "Czech Rep.", new_data$country)
# make german names -------------------------------------------------------
res = sapply(new_data$country, function(x) {
switch(
x,
"Austria" = "Österreich",
"Belgium" = "Belgien",
"Bulgaria" = "Bulgarien",
"Croatia" = "Kroatien",
"Cyprus" = "Zypern",
"Denmark" = "Dänemark",
"Estonia" = "Estland",
"Finland" = "Finnland",
"France" = "Frankreich",
"Germany" = "Deutschland",
"Greece" = "Griechenland",
"Hungary" = "Ungarn",
"Czech Rep." = "Tschechien",
"Ireland" = "Irland",
"Italy" = "Italien",
"Latvia" = "Lettland",
"Lithuania" = "Litauen",
"Luxembourg" = "Luxemburg",
"Malta" = "Malta",
"Netherlands" = "Niederlanden",
"Poland" = "Polen",
"Portugal" = "Portugal",
"Romania" = "Rumänien",
"Slovakia" = "Slovakei",
"Slovenia" = "Slowenien",
"Spain" = "Spanien",
"Sweden" = "Schweden",
)
})
new_data[["country_ger"]] = res
new_data %>%
mutate(across(where(is.numeric),
~ round(.x / 1000, 2))) -> new_data
# save data ---------------------------------------------------------------
fn = here::here("R/output/weekly_fuel_prices/weeky_fuel_prices.csv")
new_data
print(paste0("Filename: ", fn))
dir = dirname(fn); if(!dir.exists(dir)) dir.create(dir, recursive = T)
print(paste0("Created: ", dir))
write.csv(new_data, fn)
data_new
new_data
library(httr)
# url ---------------------------------------------------------------------
url = "https://www.bruegel.org/sites/default/files/2022-09/gas_tracker_update_.zip"
# download zip ------------------------------------------------------------
download_path = tempfile()
download_dir = dirname(download_path)
# download.file(url, download_path, method="libcurl", headers = c("User-Agent" = "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:104.0) Gecko/20100101 Firefox/104.0"))
httr::GET(url, write_disk(download_path, overwrite = T))
unzip(download_path, exdir = download_dir)
files = dir(download_dir)
print(files)
# find file
files = dir(download_dir, "*.csv|xlsx", full.names = T)
file_idx = which(grepl("country_data", files))
# read data
data = read.csv(files[[file_idx]])
data
# wrangle it a bit
col_indices = which(grepl("week|Russia_", names(data)))
data = data[, col_indices]
data = data[complete.cases(data), ]
data[["EU Ziel"]] = (1/3) * data$Russia_2021
print(head(data))
names(data)[names(data) == "Russia_avg"] = "Durchschnitt 2015 - 2022"
names(data)[names(data) == "Russia_2022"] = "2022"
data
head(data)
library(httr)
# url ---------------------------------------------------------------------
# url = "https://www.bruegel.org/sites/default/files/2022-09/gas_tracker_update_.zip"
url = "https://www.bruegel.org/sites/default/files/2022-10/gas%20datasets_0.zip"
# download zip ------------------------------------------------------------
download_path = tempfile()
download_dir = dirname(download_path)
# download.file(url, download_path, method="libcurl", headers = c("User-Agent" = "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:104.0) Gecko/20100101 Firefox/104.0"))
httr::GET(url, write_disk(download_path, overwrite = T))
unzip(download_path, exdir = download_dir)
files = dir(download_dir)
print("files: ", files)
library(httr)
# url ---------------------------------------------------------------------
# url = "https://www.bruegel.org/sites/default/files/2022-09/gas_tracker_update_.zip"
url = "https://www.bruegel.org/sites/default/files/2022-10/gas%20datasets_0.zip"
# download zip ------------------------------------------------------------
download_path = tempfile()
download_dir = dirname(download_path)
# download.file(url, download_path, method="libcurl", headers = c("User-Agent" = "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:104.0) Gecko/20100101 Firefox/104.0"))
httr::GET(url, write_disk(download_path, overwrite = T))
unzip(download_path, exdir = download_dir)
files = dir(download_dir)
files
# data dir ----------------------------------------------------------------
unzipped_dir_name = download_dir + "gas datasets"
# data dir ----------------------------------------------------------------
unzipped_dir_name = paste0(download_dir, "gas datasets")
unzipped_dir_name
# data dir ----------------------------------------------------------------
unzipped_dir_name = paste0(download_dir, "/gas datasets")
files = dir(unzipped_dir_name)
files
# find file
files = dir(download_dir, "*.csv|xlsx", full.names = T)
file_idx = which(grepl("country_data", files))
files
file.size()
file_idx
# read data
data = read.csv(files[[file_idx]])
data
# wrangle it a bit
col_indices = which(grepl("week|Russia_", names(data)))
library(httr)
# url ---------------------------------------------------------------------
# url = "https://www.bruegel.org/sites/default/files/2022-09/gas_tracker_update_.zip"
url = "https://www.bruegel.org/sites/default/files/2022-10/gas%20datasets_0.zip"
# download zip ------------------------------------------------------------
download_path = tempfile()
download_dir = dirname(download_path)
# download.file(url, download_path, method="libcurl", headers = c("User-Agent" = "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:104.0) Gecko/20100101 Firefox/104.0"))
httr::GET(url, write_disk(download_path, overwrite = T))
unzip(download_path, exdir = download_dir)
# data dir ----------------------------------------------------------------
unzipped_dir_name = paste0(download_dir, "/gas datasets")
files = dir(unzipped_dir_name)
# find file
files = dir(download_dir, "*.csv|xlsx", full.names = T)
files
file_idx = which(grepl("country_data", files))
# read data
data = read.csv(files[[file_idx]])
# find file
files = dir(unzipped_dir_name, "*.csv|xlsx", full.names = T)
files
file_idx = which(grepl("country_data", files))
file_idx
library(httr)
# url ---------------------------------------------------------------------
# url = "https://www.bruegel.org/sites/default/files/2022-09/gas_tracker_update_.zip"
url = "https://www.bruegel.org/sites/default/files/2022-10/gas%20datasets_0.zip"
print("here")
# download zip ------------------------------------------------------------
download_path = tempfile()
download_dir = dirname(download_path)
# download.file(url, download_path, method="libcurl", headers = c("User-Agent" = "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:104.0) Gecko/20100101 Firefox/104.0"))
httr::GET(url, write_disk(download_path, overwrite = T))
unzip(download_path, exdir = download_dir)
# data dir ----------------------------------------------------------------
unzipped_dir_name = paste0(download_dir, "/gas datasets")
print("unziiped dir: ", unzipped_dir_name)
unzipped_dir_name
library(httr)
# url ---------------------------------------------------------------------
# url = "https://www.bruegel.org/sites/default/files/2022-09/gas_tracker_update_.zip"
url = "https://www.bruegel.org/sites/default/files/2022-10/gas%20datasets_0.zip"
print("here")
# download zip ------------------------------------------------------------
download_path = tempfile()
download_dir = dirname(download_path)
# download.file(url, download_path, method="libcurl", headers = c("User-Agent" = "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:104.0) Gecko/20100101 Firefox/104.0"))
httr::GET(url, write_disk(download_path, overwrite = T))
unzip(download_path, exdir = download_dir)
# data dir ----------------------------------------------------------------
unzipped_dir_name = paste0(download_dir, "/gas datasets")
# find file
files = dir(unzipped_dir_name, "*.csv|xlsx", full.names = T)
print(paste0("files: ", files))
file_idx = which(grepl("country_data", files))
# read data
data = read.csv(files[[file_idx]])
# wrangle it a bit
col_indices = which(grepl("week|Russia_", names(data)))
data = data[, col_indices]
data = data[complete.cases(data), ]
data[["EU Ziel"]] = (1/3) * data$Russia_2021
print(head(data))
names(data)[names(data) == "Russia_avg"] = "Durchschnitt 2015 - 2022"
names(data)[names(data) == "Russia_2022"] = "2022"
data %>% tail
# read data
data = read.csv(files[[file_idx]])
data
# wrangle it a bit
col_indices = which(grepl("week|Russia_", names(data)))
col_indices
data = data[, col_indices]
data = data[, col_indices]
data[["EU Ziel"]] = (1/3) * data$Russia_2021
print(head(data))
print(tail(data))
names(data)[names(data) == "Russia_avg"] = "Durchschnitt 2015 - 2022"
names(data)[names(data) == "Russia_2022"] = "2022"
output_file = "R/output/natural_gas_russia_europe.csv"
dir = dirname(output_file); if(!dir.exists(dir)) dir.create(dir, recursive = T)
write.csv(data, output_file)
data
library(httr)
# url ---------------------------------------------------------------------
# url = "https://www.bruegel.org/sites/default/files/2022-09/gas_tracker_update_.zip"
url = "https://www.bruegel.org/sites/default/files/2022-10/gas%20datasets_0.zip"
print("here")
# download zip ------------------------------------------------------------
download_path = tempfile()
download_dir = dirname(download_path)
# download.file(url, download_path, method="libcurl", headers = c("User-Agent" = "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:104.0) Gecko/20100101 Firefox/104.0"))
httr::GET(url, write_disk(download_path, overwrite = T))
unzip(download_path, exdir = download_dir)
# data dir ----------------------------------------------------------------
unzipped_dir_name = paste0(download_dir, "/gas datasets")
# find file
files = dir(unzipped_dir_name, "*.csv|xlsx", full.names = T)
print(paste0("files: ", files))
file_idx = which(grepl("country_data", files))
# read data
data = read.csv(files[[file_idx]])
# wrangle it a bit
col_indices = which(grepl("week|Russia_", names(data)))
data = data[, col_indices]
data[["EU Ziel"]] = (1/3) * data$Russia_2021
print(head(data))
print(tail(data))
names(data)[names(data) == "Russia_avg"] = "Durchschnitt 2015 - 2022"
names(data)[names(data) == "Russia_2022"] = "2022"
# download.file(url, download_path, method="libcurl", headers = c("User-Agent" = "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:104.0) Gecko/20100101 Firefox/104.0"))
res = httr::GET(url, write_disk(download_path, overwrite = T))
res
# url ---------------------------------------------------------------------
# url = "https://www.bruegel.org/sites/default/files/2022-09/gas_tracker_update_.zip"
url = "https://www.bruegel.org/sites/default/files/2022-10/gas%20dataset_0.zip"
# download zip ------------------------------------------------------------
download_path = tempfile()
download_dir = dirname(download_path)
# Download the data in loop -----------------------------------------------
success = FALSE
i = 1
while(!success & success <= 5) {
# try the download
res = httr::GET(url, write_disk(download_path, overwrite = T))
# if it did work
if (res$status_code == 200) {
success = TRUE
}
# if it did not work
i = i + 1
}
print(paste0("Try for the ", i, ". time"))
while(!success & i <= 5) {
print(paste0("Try for the ", i, ". time"))
# try the download
res = httr::GET(url, write_disk(download_path, overwrite = T))
# if it did work
if (res$status_code == 200) {
success = TRUE
}
# if it did not work
i = i + 1
}
# Download the data in loop -----------------------------------------------
success = FALSE
i = 1
while(!success & i <= 5) {
print(paste0("Try for the ", i, ". time"))
# try the download
res = httr::GET(url, write_disk(download_path, overwrite = T))
# if it did work
if (res$status_code == 200) {
success = TRUE
}
# if it did not work
i = i + 1
}
if(!res$status_code == 200) stop("Download did not work after 5 tries")
library(httr)
# url ---------------------------------------------------------------------
# url = "https://www.bruegel.org/sites/default/files/2022-09/gas_tracker_update_.zip"
url = "https://www.bruegel.org/sites/default/files/2022-10/gas%20datasets_0.zip"
# download zip ------------------------------------------------------------
download_path = tempfile()
download_dir = dirname(download_path)
# Download the data in loop -----------------------------------------------
success = FALSE
i = 1
while(!success & i <= 5) {
print(paste0("Try for the ", i, ". time"))
# try the download
res = httr::GET(url, write_disk(download_path, overwrite = T))
# if it did work
if (res$status_code == 200) {
success = TRUE
}
# if it did not work
i = i + 1
}
if(!res$status_code == 200) stop("Download did not work after 5 tries")
unzip(download_path, exdir = download_dir)
# data dir ----------------------------------------------------------------
unzipped_dir_name = paste0(download_dir, "/gas datasets")
# find file
files = dir(unzipped_dir_name, "*.csv|xlsx", full.names = T)
print(paste0("files: ", files))
file_idx = which(grepl("country_data", files))
# read data
data = read.csv(files[[file_idx]])
data
View(data)
files
rdata = read_csv(files[5])
rdata
View(rdata)
sdata = read_csv(files[6])
View(sdata)
sdata %>% filter(country == "AT")
sdata %>% filter(country == "AT") %>% View
sdata %>% filter(country == "AT") %>% arrange(desc(dates))
sdata %>% filter(country == "AT") %>% mutate(dates = as.Date(dates))
sdata %>% filter(country == "AT") %>% mutate(dates = as.Date(dates)) %>% arrange(desc(dates)) %>% View
sdata %>% filter(country == "AT") %>% mutate(dates = as.Date(dates,)) %>% arrange(desc(dates)) %>% View
?as.Date
sdata %>% filter(country == "AT") %>% mutate(dates = as.Date(dates, format="%d/%m/%Y")) %>% arrange(desc(dates)) %>% View
files
library(httr)
library(jsonlite)
today = as.Date(Sys.time())
tomorrow = today + 1
# url ---------------------------------------------------------------------
url = sprintf("https://transparency.apg.at/transparency-api/api/v1/Data/AGPT/German/M15/%sT000000/%sT000000", today, tomorrow)
url = "https://transparency.apg.at/transparency-api/api/v1/Download/AGPT/German/M15/2021-01-01T000000/2022-09-26T000000/AGPT_2020-12-31T23_00_00Z_2022-09-26T22_00_00Z_60M_de_2022-09-26T15_40_08Z.csv"
# download zip ------------------------------------------------------------
download_path = tempfile()
download_dir = dirname(download_path)
# download data -----------------------------------------------------------
download.file(url, download_path, method="libcurl", headers = c("User-Agent" = "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:104.0) Gecko/20100101 Firefox/104.0"))
library(tidyverse)
library(lubridate)
library(here)
# dates -------------------------------------------------------------------
start = as.Date("2022-08-01")
end = Sys.time() %>% as.Date() + 1
# url ---------------------------------------------------------------------
url = "https://transparency.apg.at/transparency-api/api/v1/Download/AGPT/German/M15/2022-01-01T000000/2022-08-30T000000/868d39b3-d482-41e3-84b0-41136c8bfa68/AGPT_2021-12-31T23_00_00Z_2022-08-29T22_00_00Z_15M_de_2022-08-29T09_23_11Z.csv?"
# download data -----------------------------------------------------------
file = tempfile()
download.file(url, file)
# url ---------------------------------------------------------------------
url = "https://ec.europa.eu/energy/observatory/reports/latest_prices_with_taxes.xlsx"
# download file -----------------------------------------------------------
file = paste0(tempdir(), "/", "temp.xlsx")
download.file(url, file)
data = readxl::read_xlsx(file, sheet = 2)
# get the date (in col 5 header) ------------------------------------------------------------
col5_header = names(data)[[5]]
date_unformatted = gsub(".*\\s(\\d{1,2}\\/\\d{1,2}/\\d{1,2})$", "\\1", col5_header)
date = as.Date(date_unformatted, format="%m/%d/%y")
date
library(httr)
# url ---------------------------------------------------------------------
# url = "https://www.bruegel.org/sites/default/files/2022-09/gas_tracker_update_.zip"
url = "https://www.bruegel.org/sites/default/files/2022-10/gas%20datasets_0.zip"
# download zip ------------------------------------------------------------
download_path = tempfile()
download_dir = dirname(download_path)
# Download the data in loop -----------------------------------------------
success = FALSE
i = 1
while(!success & i <= 5) {
print(paste0("Try for the ", i, ". time"))
# try the download
res = httr::GET(url, write_disk(download_path, overwrite = T))
# if it did work
if (res$status_code == 200) {
success = TRUE
}
# if it did not work
i = i + 1
}
if(!res$status_code == 200) stop("Download did not work after 5 tries")
unzip(download_path, exdir = download_dir)
# data dir ----------------------------------------------------------------
unzipped_dir_name = paste0(download_dir, "/gas datasets")
# find file
files = dir(unzipped_dir_name, "*.csv|xlsx", full.names = T)
files
print(paste0("files: ", files))
file_idx = which(grepl("country_data", files))
# read data
data = read.csv(files[[file_idx]])
data
# wrangle it a bit
col_indices = which(grepl("week|Russia_", names(data)))
data = data[, col_indices]
data
# 20.10
url = "https://www.bruegel.org/sites/default/files/2022-10/Gas%20tracker_0.zip"
# download zip ------------------------------------------------------------
download_path = tempfile()
download_dir = dirname(download_path)
# Download the data in loop -----------------------------------------------
success = FALSE
i = 1
while(!success & i <= 5) {
print(paste0("Try for the ", i, ". time"))
# try the download
res = httr::GET(url, write_disk(download_path, overwrite = T))
# if it did work
if (res$status_code == 200) {
success = TRUE
}
# if it did not work
i = i + 1
}
if(!res$status_code == 200) stop("Download did not work after 5 tries")
unzip(download_path, exdir = download_dir)
# data dir ----------------------------------------------------------------
unzipped_dir_name = paste0(download_dir, "/gas datasets")
# find file
files = dir(unzipped_dir_name, "*.csv|xlsx", full.names = T)
print(paste0("files: ", files))
file_idx = which(grepl("country_data", files))
# read data
data = read.csv(files[[file_idx]])
data
# wrangle it a bit
col_indices = which(grepl("week|Russia_", names(data)))
data = data[, col_indices]
data
library(httr)
# 20.10
url = "https://www.bruegel.org/sites/default/files/2022-10/Gas%20tracker_0.zip"
# download zip ------------------------------------------------------------
download_path = tempfile()
download_dir = dirname(download_path)
# Download the data in loop -----------------------------------------------
success = FALSE
i = 1
library(httr)
# 20.10
url = "https://www.bruegel.org/sites/default/files/2022-10/Gas%20tracker_0.zip"
# download zip ------------------------------------------------------------
download_path = tempfile()
download_dir = dirname(download_path)
# Download the data in loop -----------------------------------------------
success = FALSE
i = 1
while(!success & i <= 5) {
print(paste0("Try for the ", i, ". time"))
# try the download
res = httr::GET(url, write_disk(download_path, overwrite = T))
# if it did work
if (res$status_code == 200) {
success = TRUE
}
# if it did not work
i = i + 1
}
if(!res$status_code == 200) stop("Download did not work after 5 tries")
unzip(download_path, exdir = download_dir)
# data dir ----------------------------------------------------------------
unzipped_dir_name = paste0(download_dir, "/gas datasets")
# find file
files = dir(unzipped_dir_name, "*.csv|xlsx", full.names = T)
files
files
print(paste0("files: ", files))
download_path
# download zip ------------------------------------------------------------
download_path = tempfile()
download_path
download_dir = dirname(download_path)
download_dir
print(paste0("Try for the ", i, ". time"))
# try the download
res = httr::GET(url, write_disk(download_path, overwrite = T))
res
if(!res$status_code == 200) stop("Download did not work after 5 tries")
unzip(download_path, exdir = download_dir)
dir(download_dir)
download_dir
dir(download_dir, ".*[gG]as.*")
dir(download_dir, ".*[gG]as.*", full.names = T)
# the unzipped file does not contain the files, but another directory which apparently
# changes each week its name. But every week so far the word "Gas" was in it
unzipped_dir_name = dir(download_dir, ".*[gG]as.*", full.names = T)
unzipped_dir_name
# find file
files = dir(unzipped_dir_name, "*.csv|xlsx", full.names = T)
files
print(paste0("files: ", files))
file_idx = which(grepl("country_data", files))
# read data
data = read.csv(files[[file_idx]])
# wrangle it a bit
col_indices = which(grepl("week|Russia_", names(data)))
data = data[, col_indices]
data[["EU Ziel"]] = (1/3) * data$Russia_2021
data
